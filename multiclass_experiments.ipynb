{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e4de7a1",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd5ba828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from siml import MultiClassClassificationSimulation\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ee56b2",
   "metadata": {},
   "source": [
    "### Parameter definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dadbd56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set numpy seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters for data generation\n",
    "n_samples = 1000\n",
    "n_features = 20\n",
    "n_classes = 3\n",
    "n_informative = 5\n",
    "n_redundant = 2\n",
    "test_training_split = 0.02\n",
    "\n",
    "# Parameters for the scheduling\n",
    "n_jobs = int(n_samples * test_training_split)\n",
    "jobs = list(range(n_jobs))\n",
    "weights = np.random.randint(10, 100, size=n_classes)\n",
    "durations = np.random.randint(1, 10, size=n_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc208d5",
   "metadata": {},
   "source": [
    "### Functions related to the scheduling problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dade74d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the scheduling problem using the STP rule\n",
    "def solve_scheduling(job_class):\n",
    "    job_weight = [weights[job_class[j]] for j in jobs]\n",
    "    ratios = [durations[j] / job_weight[j] for j in jobs]\n",
    "    solution = [x for _, x in sorted(zip(ratios, jobs), reverse=True)]\n",
    "    return solution\n",
    "\n",
    "# Compute the weighted completion time of a solution\n",
    "def compute_weighted_completion_time(job_class, solution):\n",
    "    job_weight = [weights[job_class[j]] for j in jobs]\n",
    "    completion_time = 0\n",
    "    weighted_completion_time = 0\n",
    "    for j in solution:\n",
    "        completion_time += durations[j]\n",
    "        weighted_completion_time += job_weight[j] * completion_time\n",
    "    return weighted_completion_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9ab0a9",
   "metadata": {},
   "source": [
    "### Functions related to the confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21b168eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute (macro) TPR and FPR from confusion matrix\n",
    "def compute_TPR_FPR(conf_matrix, all_classes):\n",
    "    TPR_m = {cls: 0 for cls in all_classes}\n",
    "    FPR_m = {cls: 0 for cls in all_classes}\n",
    "    for i, cls in enumerate(all_classes):            \n",
    "        TP = conf_matrix[i, i]\n",
    "        FN = conf_matrix[i, :].sum() - TP\n",
    "        FP = conf_matrix[:, i].sum() - TP\n",
    "        TN = conf_matrix.sum() - (TP + FN + FP)            \n",
    "        TPR_m[cls] = (TP / (TP + FN) if (TP + FN) > 0 else 0)\n",
    "        FPR_m[cls] = (FP / (FP + TN) if (FP + TN) > 0 else 0)\n",
    "        \n",
    "    macro_TPR_m = np.mean(list(TPR_m.values()))\n",
    "    macro_FPR_m = np.mean(list(FPR_m.values()))\n",
    "\n",
    "    if len(all_classes) == 2:\n",
    "        macro_TPR_m = list(TPR_m.values())[0]\n",
    "        macro_FPR_m = list(FPR_m.values())[0]\n",
    "    \n",
    "    return TPR_m, FPR_m, macro_TPR_m, macro_FPR_m\n",
    "\n",
    "# Generate all sum of n with n non-negative integers\n",
    "def generate_ordered_partitions(n, m):\n",
    "    result = []\n",
    "    def backtrack(path, remaining, depth):\n",
    "        if depth == m:\n",
    "            if remaining == 0:\n",
    "                result.append(path[:])\n",
    "            return\n",
    "        for i in range(remaining + 1):  # allow zero\n",
    "            path.append(i)\n",
    "            backtrack(path, remaining - i, depth + 1)\n",
    "            path.pop()\n",
    "\n",
    "    backtrack([], n, 0)\n",
    "    return result\n",
    "\n",
    "# Generate all valid confusion matrices for a given number of classes and samples\n",
    "def generate_valid_confusion_matrices(n_classes, n_per_class):\n",
    "    possible_row_values = []\n",
    "    for c in range(n_classes):\n",
    "        possible_row_values.append(generate_ordered_partitions(n_per_class[c], n_classes))\n",
    "\n",
    "    matrices = []\n",
    "    for row_0 in possible_row_values[0]:\n",
    "        for row_1 in possible_row_values[1]:\n",
    "            for row_2 in possible_row_values[2]:                 \n",
    "                matrices.append(np.array([row_0, row_1, row_2]))\n",
    "\n",
    "    return matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaa6ace",
   "metadata": {},
   "source": [
    "### Generate synthetic dataset and get the true optimal objective value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ffa8ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a multiclass classification dataset\n",
    "X, y = make_classification(n_samples=n_samples,\n",
    "                           n_features=n_features,\n",
    "                           n_classes=n_classes,\n",
    "                           n_informative=n_informative,\n",
    "                           n_redundant=n_redundant,\n",
    "                           class_sep=0.5,  # less separation between classes\n",
    "                           flip_y=0.2,  # label noise\n",
    "                           random_state=42)\n",
    "\n",
    "X = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(n_features)])\n",
    "y = pd.Series(y, name=\"class_label\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_training_split, random_state=42)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "all_classes = np.unique(y)\n",
    "n_per_class = ()\n",
    "for cls in all_classes:\n",
    "    n_per_class += (y_test.value_counts()[cls],)\n",
    "\n",
    "# Solve the optimization problem using y_test\n",
    "true_solution = solve_scheduling(y_test)\n",
    "true_optimal_makespan = compute_weighted_completion_time(y_test, true_solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fae1ee",
   "metadata": {},
   "source": [
    "### Generate all valid confusion matrices and simulate their predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07747ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to simulate all possible confusion n_classes x n_classes matrices for the y_test instances\n",
    "matrices = generate_valid_confusion_matrices(n_classes, n_per_class)\n",
    "\n",
    "for m, matrix in enumerate(matrices):\n",
    "\n",
    "    # Calculate macro TPR and FPR for the generated matrix\n",
    "    TPR_m, FPR_m, macro_TPR_m, macro_FPR_m = compute_TPR_FPR(matrix, all_classes)\n",
    "\n",
    "    # Simulate predictions using the generated TPR and FPR\n",
    "    macro_TPR_sims = []\n",
    "    macro_FPR_sims = []\n",
    "    gaps = []\n",
    "    obj_vals = []\n",
    "    for seed in range(10):\n",
    "        siml = MultiClassClassificationSimulation(seed=seed)\n",
    "        y_simulated = [siml.simulate(tc, TPR_m, FPR_m) for tc in y_test]\n",
    "        conf_matrix_sim = confusion_matrix(y_test, y_simulated, labels=all_classes)\n",
    "        TPR_sim, FPR_sim, macro_TPR_sim, macro_FPR_sim = compute_TPR_FPR(conf_matrix_sim, all_classes)\n",
    "\n",
    "        macro_TPR_sims.append(macro_TPR_sim)\n",
    "        macro_FPR_sims.append(macro_FPR_sim)\n",
    "\n",
    "        # Solve the optimization problem using y_simulated\n",
    "        sim_solution = solve_scheduling(y_simulated)\n",
    "        sim_makespan = compute_weighted_completion_time(y_test, sim_solution)\n",
    "        gap = (true_optimal_makespan - sim_makespan) / true_optimal_makespan\n",
    "        gaps.append(gap)\n",
    "        obj_vals.append(sim_makespan)\n",
    "\n",
    "    print(f\"{m};{macro_TPR_m};{np.mean(macro_TPR_sims)};{np.std(macro_TPR_sims)};{np.min(macro_TPR_sims)};{np.max(macro_TPR_sims)};{macro_FPR_m};{np.mean(macro_FPR_sims)};{np.std(macro_FPR_sims)};{np.min(macro_FPR_sims)};{np.max(macro_FPR_sims)};{np.mean(obj_vals)};{np.std(obj_vals)};{np.mean(gaps)};{np.std(gaps)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d66083",
   "metadata": {},
   "source": [
    "### Train actual models on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508ec5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGB and solve\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_xgb = xgb.predict(X_test)\n",
    "conf_matrix_xgb = confusion_matrix(y_test, y_xgb, labels=all_classes)\n",
    "TPR_xgb, FPR_xgb, macro_TPR_xgb, macro_FPR_xgb = compute_TPR_FPR(conf_matrix_xgb, all_classes)\n",
    "xgb_solution = solve_scheduling(y_xgb)\n",
    "xgb_makespan = compute_weighted_completion_time(y_test, xgb_solution)\n",
    "xgb_gap = (true_optimal_makespan - xgb_makespan) / true_optimal_makespan\n",
    "print(f\"XGB;{macro_TPR_xgb};{macro_FPR_xgb};{xgb_makespan};{xgb_gap}\")\n",
    "\n",
    "# Train LR and solve\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "y_lr = lr.predict(X_test)\n",
    "conf_matrix_lr = confusion_matrix(y_test, y_lr, labels=all_classes)\n",
    "TPR_lr, FPR_lr, macro_TPR_lr, macro_FPR_lr = compute_TPR_FPR(conf_matrix_lr, all_classes)\n",
    "lr_solution = solve_scheduling(y_lr)\n",
    "lr_makespan = compute_weighted_completion_time(y_test, lr_solution)\n",
    "lr_gap = (true_optimal_makespan - lr_makespan) / true_optimal_makespan\n",
    "print(f\"LR;{macro_TPR_lr};{macro_FPR_lr};{lr_makespan};{lr_gap}\")\n",
    "\n",
    "# Train RF and solve\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_rf = rf.predict(X_test)\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_rf, labels=all_classes)\n",
    "TPR_rf, FPR_rf, macro_TPR_rf, macro_FPR_rf = compute_TPR_FPR(conf_matrix_rf, all_classes)\n",
    "rf_solution = solve_scheduling(y_rf)\n",
    "rf_makespan = compute_weighted_completion_time(y_test, rf_solution)\n",
    "rf_gap = (true_optimal_makespan - rf_makespan) / true_optimal_makespan\n",
    "print(f\"RF;{macro_TPR_rf};{macro_FPR_rf};{rf_makespan};{rf_gap}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
